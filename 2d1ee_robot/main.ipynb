{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c793a2b6",
   "metadata": {},
   "source": [
    "## １自由度マニピュレータの動作計画\n",
    "#### 目標\n",
    "- 指定した目標角度に対して、ロボットを目標角度に動かす動作を強化学習を用いて獲得\n",
    "\n",
    "#### 問題の本質\n",
    "- 今回学習すべき**最適行動**は、現在位置とゴールの差分から、方向・距離を決定できる。  \n",
    "    - 0° -> 40°: 正方向に40°進む\n",
    "    - 270°->310°: 正方向に40°進む\n",
    "    - ⇑どちらも同じ行動として学ぶと効率的に学べそう!!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4257627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "from env import Arm1DEnv\n",
    "from visualization import ArmVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54469c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動を選択(ε-greedy法を使用)\n",
    "def choice_action(q_table, state, epsilon=0.2)->int:\n",
    "    # 探索\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(2)\n",
    "\n",
    "    # 活用\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "    \n",
    "# 今の状態と取った行動により、Q-tableを更新\n",
    "def update_q(q_table, now_state, action, next_state, reward, gamma=0.9, lr=0.05):\n",
    "    best_q = np.max(q_table[next_state])\n",
    "    # 報酬値 + 割引率 x 次の状態での最大価値\n",
    "    td_target = reward + gamma * best_q\n",
    "    q_table[now_state, action] += lr * (td_target - q_table[now_state, action])\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07525583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境の実体化\n",
    "arm_env = Arm1DEnv()\n",
    "\n",
    "# Qテーブルの初期化\n",
    "q_table = np.zeros((arm_env.n_state, arm_env.action_space), np.float32)\n",
    "\n",
    "# ハイパパラメータの設定\n",
    "# 学習エピソード\n",
    "episode = 10_000\n",
    "# 最大ステップ数\n",
    "max_step = 50\n",
    "# 割引率\n",
    "gamma = 0.9\n",
    "# 探索率\n",
    "init_epsilon = 0.15 \n",
    "# 学習率\n",
    "lr = 0.01 \n",
    "\n",
    "# log記録用\n",
    "reward_log = []\n",
    "finish_count = 0\n",
    "\n",
    "for i in range(episode):\n",
    "    total_reward = 0\n",
    "    epsilon = max(0.01, init_epsilon * (0.999 ** i))\n",
    "    for _ in range(max_step):\n",
    "        now_state = arm_env.index_state\n",
    "\n",
    "        # 行動の決定\n",
    "        action = choice_action(\n",
    "            q_table=q_table,\n",
    "            state=now_state,\n",
    "            epsilon=epsilon\n",
    "        )\n",
    "\n",
    "        # 行動->報酬・状態を取得\n",
    "        next_state, reward, is_finish = arm_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # q-tableの更新\n",
    "        q_table = update_q(\n",
    "                        q_table=q_table,\n",
    "                        now_state=now_state,\n",
    "                        action=action,\n",
    "                        next_state=next_state,\n",
    "                        reward=reward,\n",
    "                        gamma=gamma,\n",
    "                        lr=lr\n",
    "                        )\n",
    "\n",
    "        # ゴールしたらエピソード終了\n",
    "        if is_finish:\n",
    "            # print(arm_env.state, arm_env.start)\n",
    "            finish_count+=1\n",
    "            arm_env.reset()\n",
    "            break\n",
    "\n",
    "    reward_log.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a52373",
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ee002",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "moving_avg = np.convolve(reward_log, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(moving_avg)\n",
    "plt.xlabel(\"試行 (step)\")\n",
    "plt.ylabel('移動平均報酬')\n",
    "plt.title(\"報酬の推移(移動平均)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8967e2",
   "metadata": {},
   "source": [
    "#### 学習モデルを用いて可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_visu = ArmVisualizer(q_table, arm_env.angle_unit, arm_env.n_state)\n",
    "arm_visu.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76d219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
