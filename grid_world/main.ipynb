{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e136e075",
   "metadata": {},
   "source": [
    "## grid攻略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "from env import GridEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50169c",
   "metadata": {},
   "source": [
    "## アルゴリズムの流れ\n",
    "1. Qテーブル初期化\n",
    "2. 各エピソードのループ  \n",
    "   2-1. 状態sを取得  \n",
    "   2-2. ε-greedyで行動aを選択  \n",
    "   2-3. 環境を1ステップ進める → r, s' を取得  \n",
    "   2-4. Q値 Q(s, a) を更新  \n",
    "   2-5. s ← s'  \n",
    "   2-6. エピソード終了まで繰り返し  \n",
    "3. 学習の記録（報酬など）  \n",
    "\n",
    "- Q値の更新式  \n",
    "   $Q(s,a)←Q(s,a)+\\alpha[r+\\gamma \\cdot max{Q(s', a')-Q(s,a)}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019be09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動を選択(ε-greedy法を使用)\n",
    "def choice_action(q_table, state, epsilon=0.2)->int:\n",
    "    # 探索\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(4)\n",
    "\n",
    "    # 活用\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "    \n",
    "# 今の状態と取った行動により、Q-tableを更新\n",
    "def update_q(q_table, now_state, action, next_state, reward, gamma=0.9, lr=0.05):\n",
    "    best_q = np.max(q_table[next_state])\n",
    "    # 報酬値 + 割引率 x 次の状態での最大価値\n",
    "    td_target = reward + gamma * best_q\n",
    "    q_table[now_state, action] += lr * (td_target - q_table[now_state, action])\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境の実体化\n",
    "grid_env = GridEnv()\n",
    "\n",
    "# Qテーブルの初期化\n",
    "q_table = np.zeros((grid_env.size**2, grid_env.action_space), np.float32)\n",
    "\n",
    "# ハイパパラメータの設定\n",
    "# 学習エピソード\n",
    "episode = 1_000\n",
    "# 最大ステップ数\n",
    "max_step = 20\n",
    "# 割引率\n",
    "gamma = 0.9\n",
    "# 探索率\n",
    "epsilon = 0.3 \n",
    "# 学習率\n",
    "lr = 0.01 \n",
    "\n",
    "# log記録用\n",
    "reward_log = []\n",
    "\n",
    "# 実際の学習ループ\n",
    "for i in range(episode):\n",
    "    total_reward = 0\n",
    "    for _ in range(max_step):\n",
    "        now_state = grid_env.index_state\n",
    "        # 行動の決定\n",
    "        action = choice_action(\n",
    "                            q_table=q_table,\n",
    "                            state=now_state\n",
    "                            )\n",
    "\n",
    "        # 行動->報酬・状態を取得\n",
    "        next_state, reward, is_finish = grid_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # q-tableの更新\n",
    "        q_table = update_q(\n",
    "                        q_table=q_table,\n",
    "                        now_state=now_state,\n",
    "                        action=action,\n",
    "                        next_state=next_state,\n",
    "                        reward=reward,\n",
    "                        gamma=gamma,\n",
    "                        lr=lr\n",
    "                        )\n",
    "\n",
    "        # ゴールや障害物に衝突してしまったらエピソード終了\n",
    "        if is_finish:\n",
    "            grid_env.reset()\n",
    "            break\n",
    "\n",
    "    reward_log.append(total_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "moving_avg = np.convolve(reward_log, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(moving_avg)\n",
    "plt.xlabel(\"試行 (step)\")\n",
    "plt.ylabel('移動平均報酬')\n",
    "plt.title(\"報酬の推移(移動平均)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_policy_and_q_values(q_table, grid_env):\n",
    "    size = grid_env.size\n",
    "    policy = np.argmax(q_table, axis=1)\n",
    "    action_symbols = {0: '↑', 1: '→', 2: '↓', 3: '←'}\n",
    "    arrows = np.vectorize(action_symbols.get)(policy.reshape((size, size)))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # --- ① 方策の矢印マップ ---\n",
    "    axs[0].set_title(\"最適方策（argmax Q）\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            state = (i, j)\n",
    "            idx = i * size + j\n",
    "            if state in grid_env.holes:\n",
    "                axs[0].text(j, i, '■', ha='center', va='center', fontsize=18)\n",
    "            elif state == grid_env.goal:\n",
    "                axs[0].text(j, i, 'G', ha='center', va='center', fontsize=18)\n",
    "            else:\n",
    "                axs[0].text(j, i, arrows[i, j], ha='center', va='center', fontsize=18)\n",
    "    axs[0].set_xlim(-0.5, size - 0.5)\n",
    "    axs[0].set_ylim(size - 0.5, -0.5)\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].set_aspect('equal')\n",
    "\n",
    "    # --- ② Q値ヒートマップ（最大Q値のみ表示） ---\n",
    "    max_q_values = np.max(q_table, axis=1).reshape((size, size))\n",
    "    im = axs[1].imshow(max_q_values, cmap='YlGnBu')\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            axs[1].text(j, i, f\"{max_q_values[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    axs[1].set_title(\"Q値の最大値ヒートマップ\")\n",
    "    axs[1].set_xticks(range(size))\n",
    "    axs[1].set_yticks(range(size))\n",
    "    axs[1].set_aspect('equal')\n",
    "\n",
    "    fig.colorbar(im, ax=axs[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_policy_and_q_values(q_table, grid_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31255f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
